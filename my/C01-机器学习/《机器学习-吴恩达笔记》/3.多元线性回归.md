- 下边我们介绍多个特征量的线性回归形式，并通过向量乘法表示。例如，之前的预测房价例子中，我们只有一个特征向量（房屋大小），来预测房屋价格。但是，实际上房屋价格不仅仅与大小有关，还与卧室数量、楼层数、使用年限等多个特征向量有关

##### 相关符号含义

![QQ图片20200731191410.png](../../img/bVbKz05)
在多元线性回归中，m仍表示数据集的数据数量，上标i表示第i个训练样本；n表示每个数据的特征值数量，下标表示数据的第i个特征

##### 多元假设函数的引入

![QQ图片20200731191620.png](../../img/bVbKz1d)

- 例如在房价预测的例子中，可用上图的多元线性函数作为假设函数。随着面积、楼层数、此房屋所在楼层增大，房价增大；随着房屋年限增大，房价变小

##### 用向量形式表示多元假设函数

- 设x0=1，即定义了额外的特征量，但取值总为1.这样特征量可用一个n+1维向量表示，同样我们把参数看做n+1维的行向量。两个向量相乘，就得到我们的假设函数

![QQ图片20200731192031.png](../../img/bVbKz1X)

### 多元线性回归的梯度下降法

- 前边我们提到了多元线性回归的假设形式，下边我们看一下如何找到满足假设最拟合的参数。即如何使用梯度下降法，解决多特征的线性回归问题
- 多元线性回归的梯度下降法与一元线性回归类似，都是先写出代价函数式子，然后将代价函数代入梯度下降公式中，经过计算得到参数j的偏导数项。不断的用参数j，减去学习速率a*参数j的偏导数，直到参数收敛

![QQ图片20200731193056.png](../../img/bVbKz3e)

- 多元梯度下降的参数更新规则实际上和一元梯度下降也是相同的。只是把x0设为1而已

![QQ图片20200803090322.png](../../img/bVbKEiG)

### 梯度下降算法的使用技巧

- 在学习完多特征的线性回归模型后，我们来看一下梯度下降算法的使用技巧

##### 梯度缩放

- 当机器学习问题有多个特征时，若特征都处于相近的范围内，梯度下降算法会更快地收敛。若特征范围相差较大，如预测房价问题中，面积大小在0~2000间，卧室数目在1~5之间，那么轮廓图就变成了非常瘦长的椭圆（类似地形图，卧室数目单位影响更大，所以变化更快，而地形图中变化快的线密集）。这个时候我们使用梯度下降法，就会来回波动，下降缓慢

![QQ图片20200731200820.png](../../img/bVbKz8z)
这样的情况下，我们就会进行特征缩放，让特征的范围尽可能接近。这样轮廓图就会就会接近圆，不再是瘦长的椭圆。这时梯度下降的速率就会变快

- 一般来说，特征缩放将特征缩放到-1和1之间。但是实际上范围可以大于小于这个范围，只要别太过分就行

![QQ图片20200731200843.jpg](../../img/bVbKz8B)
如上图，0~3、-2~0.5是可以接受不用缩放的，但是-100~100、0.001就相差太远，需要用特征缩放。一般来说，-3~3和-1/3~1/3是可以接受的，超出就要考虑特征缩放

##### 均值归一化

- 除了梯度学习，也可以通过均值归一化，即让特征值具有0的平均值，来让梯度下降更快。减去平均值，除以范围（最大值-最小值）。不一定完全精确，大概就行

![QQ图片20200731200901.png](../../img/bVbKz8I)

### 如何选择合适的特征和假设函数

- 前边学习了多特征的线性回归，下边我们来看一下如何选择合适的特征或者方法

##### 选择合适的特征：可以自己创造新的特征

- 在预测房价的例子中，若给予宽度深度两个特征，而真正确定房价的特征应该是面积。所以我们用深度*宽度，得到新的特征面积，用一元线性回归进行预测

![QQ图片20200801091123.png](../../img/bVbKAIQ)

即我们不是给什么特征就用什么特征，而是从特征和标签的角度审视问题，必要时通过定义新的特征，得到更好地模型

##### 选择合适的假设函数：了解各种函数的走势

- 在机器学习中，有时线性函数不能很好的拟合数据，可能要用到非线性函数。如预测房价中，显然直线是不能拟合的；若使用二次函数的话，后边会降回来，但是房价不会因为面积过大降回来，不符合实际；因此我们考虑使用三次函数拟合
  ![QQ图片20200801092814.png](../../img/bVbKAJW)

  ### 正规方程：求最优解参数的另一种方法

  - 在前边的线性回归问题中，我们一直使用梯度下降法求假设函数的最优解参数，这种方法需要通过多次迭代，来收敛到参数的全局最小值。这里，我们再引入正规方程的方法，通过矩阵计算，一步得出参数的最优解

##### 先对正规方程有一个直观理解

![QQ图片20200801103818.png](../../img/bVbKASL)

- 先从一个参数看起，求最优解，就是求导，然后导数置零得到的参数值
- 当有多个参数时，对每个参数求偏导数，全部置零时得到的每个参数的值。但是偏微分方法太过复杂，因此我们不使用遍历微分的方法，而是通过矩阵计算得到参数值
- 假设有m=4个样本数据，首先构建前边的x矩阵。每个样本给出自己的特征向量（仍然x0=1），转置后，将第一个数据的向量作为第一行，第二个数据的向量作为第二行，以此类推。然后进行下边的矩阵计算，得到的结果就是参数值

![QQ图片20200801104151.png](../../img/bVbKAS9)

##### 正规方程和梯度下降的比较

![QQ图片20200801184332.png](../../img/bVbKB1N)

- 梯度下降法：

（1）若参数不在相近的范围内，需要梯度缩放或者特征归一化，处理到相近范围内；特征方程则不需要
（2）需要人为选择学习速率，尝试不同的学习速率，运行多次找到最好的那个，带来了额外的工作和麻烦
（3）需要多次迭代，计算慢

- 特征方程：

（1）不需要选择学习速率
（2）运行一次即可
（3）不需要画出曲线检查收敛性

- 看了上边，似乎正规方程远优于梯度下降。但是，特征方程需要样本数目m和特征量n值比较小。由于矩阵运算相当于矩阵维度的三次方计算，当m或n太大时，计算会非常慢，还不如用梯度下降快。一般，我们把一万作为分界点，超过一万就只考虑梯度下降法，一万以内正规方程比较好

而且随着算法越来越复杂，后边的分类算法等只能用梯度下降，不能用特征方程解决
\##### 使用正规方程的问题：矩阵的不可逆性

- 当使用正规方程时发现矩阵不可逆怎么办？通常由两种原因引起

![QQ图片20200801190036.png](../../img/bVbKB29)

（1）某些特征间存在一个固定关系。例如预测房价中，把面积英寸和平方米作为特征，两者存在3.28的换算（线性）关系，这会造成矩阵的不可逆
（2）特征数量远大于数据个数。假设我们有10个训练样本，却有100个特征值，要从10个样本中找到100个参数值，太困难。可能造成矩阵的不可逆。对于m>>n的问题，我们可以通过正则化的线性代数方法，删除某些重复或低效特征解决m>>n的问题