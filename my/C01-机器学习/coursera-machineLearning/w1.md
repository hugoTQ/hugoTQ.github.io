

# week1

- 区分监督学习与无监督学习
- 区分回归问题和分类问题

- Hypothesis：假设



- 代价函数：
  
- 
  
- 梯度下降的本质？

  ![1577618004340](../../../../../projectdocsmyimages/1577618004340.png)

  - 确定最低点方向，斜率为正数，表示最低点在更小，w-斜率*学习速率，反之，斜率为负，最低点在右侧
  - 确定步幅，越接近最低点，斜率的绝对值更小，更小的步幅接近最低点

- 线性回归的代价函数都是凸函数，即局部最优解就是全局最优解

![1577619573764](../../../../../projectdocsmyimages/1577619573764.png)

- 批量梯度下降

 每一次梯度下降，m是全部样本，计算使用所有样本计算![1577620739356](../../../../../projectdocsmyimages/1577620739356.png)



- 向量：只有一列的矩阵。

  ![1577621755846](../../../../../projectdocsmyimages/1577621755846.png)

- 矩阵与向量相乘

![1577622406406](../../../../../projectdocsmyimages/1577622406406.png)

- 矩阵相乘

  ![1577622910276](../../../../../projectdocsmyimages/1577622910276.png)

- 单位矩阵

  - 矩阵相乘不符合结合律和交换律，单位矩阵则符合

  ![1577623726175](../../../../../projectdocsmyimages/1577623726175.png)

- 逆矩阵：矩阵与自身逆矩阵相乘等于单位矩阵

  - 没有逆矩阵的矩阵叫做奇异矩阵

  ![1577624226432](../../../../../projectdocsmyimages/1577624226432.png)