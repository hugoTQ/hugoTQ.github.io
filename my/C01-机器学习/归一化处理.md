**归一化的两个好处**

1. 提高模型的收敛速度

   如下图，x1的取值为0-2000，而x2的取值为1-5，假如只有这两个特征，对其进行优化时，会得到一个窄长的椭圆形，导致在梯度下降时，**梯度的方向为垂直等高线的方向而走之字形路线**，这样会使迭代很慢，相比之下，右图的迭代就会很快（理解：也就是步长走多走少方向总是对的，不会走偏）



2. 提升模型的精度

归一化的另一好处是提高精度，这在涉及到一些距离计算的算法时效果显著，比如算法要计算欧氏距离，上图中x2的取值范围比较小，涉及到距离计算时其对结果的影响远比x1带来的小，所以这就会造成精度的损失。所以归一化很有必要，他可以让各个特征对结果做出的贡献相同。









数据归一化问题是数据挖掘中特征向量表达时的重要问题，当不同的特征成列在一起的时候，由于特征本身表达方式的原因而导致在绝对数值上的小数据被大数据“吃掉”的情况，这个时候我们需要做的就是对抽取出来的features vector进行归一化处理，以保证每个特征被分类器平等对待。下面我描述几种常见的Normalization Method，并提供相应的python实现（其实很简单）：

1、(0,1)标准化：

这是最简单也是最容易想到的方法，通过遍历feature vector里的每一个数据，将Max和Min的记录下来，并通过Max-Min作为基数（即Min=0，Max=1）进行数据的归一化处理：



 

LaTex：{x}_{normalization}=\frac{x-Min}{Max-Min}

Python实现：

def MaxMinNormalization(x,Max,Min):

```python
x = (x - Min) / (Max - Min);
 
return x;
```
找大小的方法直接用np.max()和np.min()就行了，尽量不要用python内建的max()和min()，除非你喜欢用List管理数字。

 

2、Z-score标准化：

这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，这里的关键在于复合标准正态分布，个人认为在一定程度上改变了特征的分布，关于使用经验上欢迎讨论，我对这种标准化不是非常地熟悉，转化函数为：



 

LaTex：{x}_{normalization}=\frac{x-\mu }{\sigma }

Python实现：

def  Z_ScoreNormalization(x,mu,sigma):

```python
x = (x - mu) / sigma;
 
return x;
```
这里一样，mu（即均值）用np.average()，sigma（即标准差）用np.std()即可。




3、Sigmoid函数

Sigmoid函数是一个具有S形曲线的函数，是良好的阈值函数，在(0, 0.5)处中心对称，在(0, 0.5)附近有比较大的斜率，而当数据趋向于正无穷和负无穷的时候，映射出来的值就会无限趋向于1和0，是个人非常喜欢的“归一化方法”，之所以打引号是因为我觉得Sigmoid函数在阈值分割上也有很不错的表现，根据公式的改变，就可以改变分割阈值，这里作为归一化方法，我们只考虑(0, 0.5)作为分割阈值的点的情况：

 





 

LaTex：{x}_{normalization}=\frac{1}{1+{e}^{-x}}

Python实现：

def sigmoid(X,useStatus):

```python
if useStatus:
 
    return 1.0 / (1 + np.exp(-float(X)));
 
else:
 
    return float(X);
```
这里useStatus管理是否使用sigmoid的状态，方便调试使用。

函数的基本性质：

定义域：(−∞,+∞)(−∞,+∞)
值域：(−1,1)(−1,1)
函数在定义域内为连续和光滑函数
处处可导，导数为：f′(x)=f(x)(1−f(x))f′(x)=f(x)(1−f(x))
最早Logistic函数是皮埃尔·弗朗索瓦·韦吕勒在1844或1845年在研究它与人口增长的关系时命名的。广义Logistic曲线可以模仿一些情况人口增长（P）的 S 形曲线。起初阶段大致是指数增长；然后随着开始变得饱和，增加变慢；最后，达到成熟时增加停止。
------------------------------------------------
版权声明：本文为CSDN博主「Danker01」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_42575020/article/details/82944291